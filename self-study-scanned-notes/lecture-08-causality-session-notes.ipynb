{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Causal discovery and causal inference.**\n",
    "* Former is how to discover causal information from observational data using statistical analysis.\n",
    "* Latter is how to idewntifty the caausal effect of some variable on another.\n",
    "* Both are complementary. One is way of finding causal info, other is way of finding causal effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Causality vs dependence.**\n",
    "* Maybe causality implies correlation, but correlation does not directly imply causality.\n",
    "* Association and dependence are very different - the slides make a distinction.\n",
    "* Association - through independence. Two variables have different distributions.\n",
    "* Causality - defined in terms of interventions in terms of do calculus.\n",
    "* Intervention requires all other variables to be unchanged (ceteris paribus).\n",
    "* If you can observe different distributions of Y, after the intervention on X; then X must have a way to influence Y.\n",
    "* Causality requires intervention.\n",
    "* But definition is a little circular. Without knowing anything about causal relations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Causal discovery from data.**\n",
    "\n",
    "* Taking association as causality leads to issues.\n",
    "* Causality and goal-directed behaviour are twins, in opposition to association.\n",
    "* KZ emphasises the role of intervention (this is in the spirit of Pearl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Causal thinking**\n",
    "\n",
    "**Causal effect identification**\n",
    "\n",
    "**Causal discovery**\n",
    "\n",
    "* A lot of material - read in line with what you are interested in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Causal Thinking (1)**\n",
    "\n",
    "* Distinguish causal connections and associational information.\n",
    "* Example by Fisher in the 1950s. Smoking is common cause, yellow fingers and lung cancer are correlated (due to smoking).\n",
    "* Changing incidence of lung cancer cannot be achieved by e.g. changing fingernail colour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Causal Thinking (2)**\n",
    "\n",
    "**Simpson's Paradox (1970s) - stone size, treatment, recovery.**\n",
    "* If you look at treatment A, for both large and small stones, the recovery rate is higher than treatment B individually. So we might conclude that treatment A is better for each group, if we only cared about rtecovery rate.\n",
    "* Combine two groups. Treatment B is better.\n",
    "* If you are a doctor and only care about the recovery rate of the patients. A new patient comes to you. Do you recommend  treatment A or B?\n",
    "    * W: Depends on their risk attitudes.\n",
    "* KZ: Issue - Associational. Only relying on conditional distributions. The stone is either small or large. If small recommend A, if large recommend A, if you don't know recommend B.\n",
    "\n",
    "**We care about causal effect of treatment on recovery. We make decisions based on this link. We are not interested of associational effects of stone size and recovery. We don't change stone size, it is given. BEcause of the common cause, dependence patterns between variables can be almost arbitrary.** \n",
    "\n",
    "**Cholesterol example.**\n",
    "\n",
    "* In each group, more exercise means lower cholesterol.\n",
    "* But if we put them all together, there is a positive correlation between exercise and cholesterol. \n",
    "* Issue is that we can rely on these pictures if we want to change somehting/make interventions. For pure prediction, we only care about conditioonal distriubution. So we must fix age.\n",
    "\n",
    "**Strange dependence patterns**\n",
    "\n",
    "* Female students on avergae smarter than male students in America? MIght believe intelligence to be be equally distributed. Gender, IQ, College. V-structure. Given common effect of college, gender and IQ become dependent. \n",
    "* Polling data very different from final outcome due to selection bias\n",
    "* Monty Hall problem - needs some deeper reflection.\n",
    "\n",
    "* Decisions need us to pay attention to causality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Causal Models?**\n",
    "\n",
    "* DAG. Edge is directed causal influence.\n",
    "* To represent intervention, change the variable of interest (X3), and cut off edges prior to it. Construct a new graph.\n",
    "* To see the effect of this change, we then do probabilistic inference on the new graph. \n",
    "* If there are causal relations, there is a modularity property from the DAG representation.\n",
    "* Certain causal relations forbid intervention, but we may still be interested in a causal picture.\n",
    "\n",
    "* **Causal models contain a compact description othe properties of the joint distribution, and how it can change in different scenariou.**\n",
    "* Modularity implies the an idea of local causality. \n",
    "* When Y causes X, Y must change X ontologically step by step..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conditioning, Manipulating, Counterfactual Thinking**\n",
    "* 3 questions in CS/AI:\n",
    "* **Prediction** - Fundamentally interested in conditional distribution, can be inferred from observational data.\n",
    "* **Intervention** - Would the pavement be slippery if we make sure that the sprinkler is off? Causal/interventional effect of sprinkler on state of pavement. Observational data can help, cut without knowinng anything about causal relations, we can't make any progress. There is causal asymmetry in this representation. \n",
    "* **Counterfactual** - Would the pavement be slippery had the sprinkler been off, given that the pavement is in fact not slippery and the sprinkler is on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identification of causal effects**\n",
    "\n",
    "* Classifcal problem in statistics (called causal inference).\n",
    "* Distinction between DAGS and causal DAGS.\n",
    "* We want to make adjustments to make sure causal DAGS represent causal info.\n",
    "* DAGS/BNs - mostly used to encode properties of distribution e.g conditional independencies properties, compact way to represent joint disrtibution.\n",
    "* Causal DAGS - we want them to be able to represent further properties.\n",
    "* Each edge represents fixed casual influence. \n",
    "* To ensure a causal DAG, we do it in terms of formal conditions - consider slides.\n",
    "\n",
    "* Consider a probability distribution resulting from an intervention. \n",
    "1) Markov relative to G - For this distribution after intervention, I can still factorise the distribution according to the graph, as product of individual probaiblities of each variable given its parents.\n",
    "2) For the variables on which I do interventions, their values are fixed. Have  a set of variables which you intervene on, \n",
    "3) After you do the intervention, for al; the other variables, their conditional distributon for the variables you don't intervene on would be the same (modularity).\n",
    "\n",
    "**Satisfaction of these conditions yields a causal interpretation of the DAG.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structural causal models.**\n",
    "\n",
    "* A way to assign value to variable on the LHS of the equation. No exchangeability under intuition. That is if one moves a variable from RHS to LHS, it is an algebraic relation, but no longer a **structural causal model** endowed with the corresponding semantics and intuition.\n",
    "\n",
    "* Modularity and autonomy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identification of causal effects**\n",
    "\n",
    "* Intervention\n",
    "* Traditionally, use RCTs.\n",
    "* Two groups, A and B.\n",
    "* All factors that influence outcomes across two groups are either fixed or vary at random, so any changes in trhe outcome variable must be due to the controlled variable. \n",
    "* Have to collect lots of subjects. Above condition must hold - how to control for all these covariates.\n",
    "* Make use of causal information and observational data to make inferences instead.\n",
    "\n",
    "* Distinction between prediction and intervention!\n",
    "* Prediction uses product and sum rule to get conditional probability of recovery given treatment.\n",
    "\n",
    "* Causal effect - probabiliy of recovery given that we set the value of treatment.\n",
    "* Cut off edges of parents into the treatment variable - that is stone size.\n",
    "* After the cut-off, stone size will become indepdecent. I am assigning value to treatment indepedently of stone size. Replace P(S|T) with P(S).\n",
    "* That is distinction between conditioning and manipulating.\n",
    "* Mathematical differences between manipulating and conditioning - use marginal probability of S rather than conditional of S|T."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Causal effects**\n",
    "\n",
    "* Do calculus definition of causal effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identifiability**\n",
    "\n",
    "* Causal structure vs observational data (for some of the variables). Eg. treatment and recovery but not stone size.\n",
    "* Stone size is a confounder, as it is latent direct common cause of the two variables. Also not observable.\n",
    "* 3 variables in causal structure, observational data only has 2 data on 2 variables in this example.\n",
    "\n",
    "* Have two \"models\". Causal structure and distribution of some variables implied by observational data.\n",
    "* Will the two models give same estimate of the causal effect, i.e. same conditional distir Y | do (X). \n",
    "* If yes then the causal effect is identificable. Information can give unique anaswer to question. Otherwise not. \n",
    "\n",
    "* Suppose we don't have stone size as a confounder. I.e. We have a two variable model and full observational data for X and Y.\n",
    "\n",
    "\n",
    "* **Q: Is the causal effect of treatment on recovery P(Y|do(X)) identifiable from the structure and the observational data?**\n",
    "\n",
    "* W: Yes. IF you are sure there are no confounders then there is a one to one mapping given the data of the relationship which can be inferred. If there are confounders, then it introduces an extra degree of freedom, and causal effect is NOT identifiable parametrically.\n",
    "* KZ : Yes. With no confounder P(Y|do(X) = P(Y|X).  A regression problem...\n",
    "\n",
    "\n",
    "* **Q: In presence of unobserved confounder C, is the causal effect P(Y|do(X) identifiable?**\n",
    "\n",
    "* KZ: Introduce directed common cause C (stone size) which is unobservable. Remember Simpson's paradox, with a confounder, we can have different models with the same stucture and distribution for X, Y, but not C; which will give different estimates for quantity P(Y|do(X)). Two scenarious. We know marginal distrib of X, Y. Suppose links between C are very very weak. Then it collapses into 1st situation. \n",
    "* KZ: Second scenario. Suppose C has very heavy influence on X and Y. Simpson's Paradox example - we only observe associational information between X and Y. We clearly cannot identify the causal coefficient nor determine it. When we say causal effect, we care about causal coefficeint, if it is large, we say causal effect is large. Cannot identify it only from distribution of X and Y if we have a C variable as a common cause. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identifiability of causal effects**\n",
    "\n",
    "* KZ covered this quickly.\n",
    "\n",
    "* Based on graphical criteria (Judea Pearl).\n",
    "* Back-door criterion - Only care about back-door paths into X_i\n",
    "* Back-door adjustment - similar. Replace conditional probability with a marginal for P(Y|do(X) in the sum-product.\n",
    "* IF Z satisifes the back-door crtierion, then we can identify the causal effect.\n",
    "* Using this reasoning - stone-size/C satisfies the back-door criterion; so we can estimate the causal effect of X on Y.\n",
    "\n",
    "* X_3 and X_4 satisfy the back-door criteria.\n",
    "\n",
    "* Front-door criterion. KZ skips results, can read up on them. E.g. identifying causal effect of smoking, relation between potential outcome and graphical critieria framework.\n",
    "\n",
    "* Unification - necessary and sufficeint conditons for causal effects to be identifiable. Parametric and non-parametric assumptions - instrumental variables in economics. In non-parametric settings, generally cannot identify, due to bi-directed edges/confounders.\n",
    "\n",
    "* Calculation of causal effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Causal discovery**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constraint-based causal disvovery**\n",
    "\n",
    "* Meant conditional independence constraints, in last 20 years, non-Gaussian methods were discovered. Under mild assumptions causal DAG can be recovered uniquely.\n",
    "* How to find causal information?\n",
    "* To identify causal effect, you need access to causal structure, otherwise you have no idea about causal influence.\n",
    "* We have to discover it from purely observational data.\n",
    "* Traditional way - use intervention. Do intervention, observe effect of intervention/manipulation. \n",
    "* Changing bus timetable is not a valid intervention in the bus example. It is hard to do it without understanding the system.\n",
    "* We can comment because we already know the intervention. \n",
    "* Without knowing system properties (through common sense understanding), it is difficult to find correct interventions.\n",
    "\n",
    "* Paper example\n",
    "* Not possible to do intervention in study, have to wait 2000 years to do intervention.\n",
    "* So have to analyse observational data. They found X and Y are dependent, and also dependent given any Z.\n",
    "\n",
    "**Constraint based causal discovery - via CI constraints.**\n",
    "* Find a DAG or set of DAGS that satisfy CI constraints.\n",
    "* **Assume Markov condition (graph property)**\n",
    "* **Assume faitfhulness assumption**\n",
    "* If we just *to learn BN network structure with no causal interpretation we can avoid faithfulness assumption*, we are not talking about ground truth behind the data, just encoding property of the distribution.\n",
    "* However, if we want to *guarantee that info from statistical properties of data corresponds to an underlying ground truth*, we need to assume faithfulness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Going from CI to Graph**\n",
    "\n",
    "* We have independence constraints, remove edges due to conditional indepecence, find direction of edges, do orientation progapgation.\n",
    "* Have to esablish relation between conditional indepdence in the data; and also in the structure of the DAG (causal sturcture)\n",
    "* DAG Structure gives us independence properties (d-separation)\n",
    "* We can verify these independence properties in the data using statistical tests.\n",
    "* How do we go in reverse? I.e. Find simple graph given the data.\n",
    "* Contrapositive - dependence means that two variables are not d-separated. Markov condition is not sufficient if I want to say something about structure.\n",
    "* We assume that all indepedence properties in the data we observe are entailed by the Markov condition on the graph. \n",
    "* If they are conditionally independent given Z, then they are d-separated.\n",
    "* **Markov condition allows us to move from causal DAG structure to statitical independence properties of the data.**\n",
    "* **Faithfulness allows us to move the other way from CI independences to causal structure.**\n",
    "* This is the issue of causal structures vs statisical independences. \n",
    "\n",
    "* Adjacent variables cannot be d-separated.\n",
    "* Certain independence relationships are not entailed by the Markov condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussions**\n",
    "\n",
    "* Can we find the skeleton fothe casusa structure - above.\n",
    "* Can we determine the causal direction - yes.\n",
    "* Decided to pause on more comprehensive noting as KZ was moving much more quickly through slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confounders**\n",
    "\n",
    "* In many situations there will be confounders. \n",
    "* The example Kun gives is quite powerful. On the possibility of a confounder ruled out by an application of d-separation and conditional independence relationships. And being able to say one variable directly causes another. \n",
    "* In another example, we can state there must be confounders, and that no causal influence between X_2 and X_4; X_3 is not an effect of X_4. \n",
    "* Allows us to say something about the existence of confounders, and causal influence using FCI and PC algorithms (to infer properties of confounders from CI properties)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical issues**\n",
    "\n",
    "* Examples of complexities encountered (statistical ones) when dealing with real data.\n",
    "* To do reliable causal discovery. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
